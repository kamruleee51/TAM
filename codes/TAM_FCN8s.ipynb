{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58654a37",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from monai.utils import set_determinism, first  # Utility functions from MONAI, including setting random seed for reproducibility and retrieving the first item from an iterable\n",
    "from monai.transforms import (  # Importing MONAI data transformations used for preprocessing and augmentation\n",
    "    EnsureChannelFirstD,  # Ensures image has a channel-first format, necessary for PyTorch models\n",
    "    Compose,  # Composes multiple transformations into a single callable transform\n",
    "    LoadImageD,  # Loads medical images from file, supporting multiple formats like NIfTI, PNG, and DICOM\n",
    "    RandRotateD,  # Applies a random rotation transformation to images, useful for data augmentation\n",
    "    RandZoomD,  # Applies a random zoom transformation, helping with scale-invariant learning\n",
    "    ScaleIntensityRanged  # Normalizes image intensities within a given range, improving model performance\n",
    ")\n",
    "\n",
    "import monai  # Import MONAI framework for medical imaging AI, including deep learning utilities\n",
    "from monai.data import DataLoader, Dataset, CacheDataset  # Data handling classes from MONAI, with caching for performance improvement\n",
    "from monai.config import print_config, USE_COMPILED  # Print MONAI config details and check if compiled layers are used for performance\n",
    "from monai.networks.nets import *  # Import MONAI's neural network architectures, including U-Net, SwinUNETR, etc.\n",
    "from monai.networks.blocks import Warp  # Warp block for spatial transformations, useful for registration and motion estimation\n",
    "from monai.apps import MedNISTDataset  # Pre-loaded medical imaging dataset for experimentation and benchmarking\n",
    "import torch.nn.functional as F  # Import PyTorch functional utilities for operations like activation functions and loss calculations\n",
    "\n",
    "from torchinfo import summary  # Utility for model summary visualization, displaying layer-wise structure and parameter counts\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis  # Tool for analyzing floating point operations (FLOPs), helpful for model complexity estimation\n",
    "\n",
    "from glob import glob  # Module for file path pattern matching, useful for loading datasets from directories\n",
    "import cv2  # OpenCV for image processing, commonly used for visualization and preprocessing\n",
    "import torchmetrics  # PyTorch Metrics for model evaluation, providing a variety of metrics for classification and segmentation\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter  # TensorBoard logging utility for monitoring training progress and performance metrics\n",
    "\n",
    "from torch.autograd import Variable  # Autograd wrapper for automatic differentiation, useful for tracking gradients\n",
    "\n",
    "from scipy.spatial.distance import directed_hausdorff  # Function to compute Hausdorff distance, measuring shape similarity\n",
    "import pandas as pd  # Pandas for data handling and analysis, commonly used for logging results\n",
    "import torch.nn as nn  # PyTorch's neural network module, providing layers like Conv2D, Linear, BatchNorm, etc.\n",
    "\n",
    "import numpy as np  # NumPy for numerical computations, widely used for array manipulations\n",
    "import torch  # PyTorch main package for deep learning and tensor computations\n",
    "from torch.nn import MSELoss  # Mean Squared Error loss function, commonly used for regression tasks\n",
    "import matplotlib.pyplot as plt  # Plotting library for visualizing data, including loss curves and predictions\n",
    "import os  # OS module for file handling, useful for managing datasets and saving models\n",
    "import tempfile  # Utility for handling temporary files, often used in testing and caching\n",
    "from monai.losses import *  # MONAI loss functions, including DiceLoss, FocalLoss, etc., for medical imaging segmentation\n",
    "from monai.metrics import *  # MONAI metrics for model evaluation, such as Dice Score and Hausdorff Distance\n",
    "from piqa import SSIM  # Structural Similarity Index (SSIM) for image quality assessment, measuring perceived similarity\n",
    "import visdom  # Visualization tool for real-time data visualization, useful for monitoring model training\n",
    "from tqdm import tqdm  # Progress bar utility, helpful for tracking training and data loading progress\n",
    "\n",
    "import torch  # Re-importing PyTorch (redundant, but common in large projects)\n",
    "import torch.nn as nn  # Re-importing PyTorch NN module (also redundant)\n",
    "import torch.optim as optim  # Optimizers for training neural networks, including Adam, SGD, and RMSprop\n",
    "from torch.utils.data import DataLoader  # DataLoader for batch processing and efficient data handling\n",
    "from torchvision import transforms  # Torchvision transformations for image processing, useful for augmentations\n",
    "# import torchio as tio  # (Commented out) TorchIO for medical image augmentation and preprocessing (not currently used)\n",
    "import nibabel as nib  # NiBabel for handling medical imaging formats (NIfTI, DICOM, Analyze, etc.)\n",
    "from helper import * # Load custom helper\n",
    "import config  # Importing configuration file (assumed to be custom), likely containing hyperparameters and paths\n",
    "print_config()  # Print MONAI configuration details, helping with debugging and environment setup\n",
    "set_determinism(42)  # Set random seed for reproducibility, ensuring consistent experimental results\n",
    "import torch, torchinfo  # Re-importing PyTorch and torchinfo (redundant)\n",
    "from torchviz import make_dot, make_dot_from_trace  # Visualization tools for neural networks, creating computational graphs\n",
    "from helper import make_one_hot  # Custom helper function to convert labels to one-hot encoding, useful for classification and segmentation\n",
    "\n",
    "import albumentations as A  # Albumentations library for image augmentation, providing advanced transformations\n",
    "from albumentations.pytorch import ToTensorV2  # Convert Albumentations output to PyTorch tensor, ensuring compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094c459",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Checking the number of GPUs available in the system\n",
    "# torch.cuda.device_count() returns the total number of GPUs in the system.\n",
    "print('How many GPUs = ' + str(torch.cuda.device_count()))\n",
    "\n",
    "# Checking the availability of CUDA-enabled device (GPU)\n",
    "# If a GPU is available, the device will be set to 'cuda:0', otherwise it will default to 'cpu'.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Printing the device being used (either CUDA or CPU)\n",
    "print(device)\n",
    "\n",
    "# If CUDA is not available (i.e., no GPUs), an exception is raised with a warning message.\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU not available. CPU training will be too slow.\")\n",
    "\n",
    "# Printing the name of the CUDA device (GPU) at index 0, if available\n",
    "# torch.cuda.get_device_name(0) returns the name of the GPU device at index 0.\n",
    "print(\"device name\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80dee8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Set the number of intermediate frames and the number of attention heads for multi-head attention (MH).\n",
    "num_mid_frames = 3\n",
    "num_heads_ = 8\n",
    "\n",
    "# Check if the number of intermediate frames is None\n",
    "if num_mid_frames is None:\n",
    "    # If num_mid_frames is None, set saveFile name and directory path for the data (assuming default 0 for mid frames).\n",
    "    saveFile = 'FCN8s_TAM_MH_' + str(num_heads_) + '_Mid_' + str(0)\n",
    "    data_dir = 'data/CAMUS_data/'  # Default data directory path when num_mid_frames is None.\n",
    "    print(data_dir) \n",
    "\n",
    "# If num_mid_frames is not None (i.e., there are intermediate frames to consider)\n",
    "if num_mid_frames is not None:\n",
    "    # If num_mid_frames is provided, create the saveFile name and the directory path for data based on the number of frames.\n",
    "    saveFile = 'FCN8s_TAM_MH_' + str(num_heads_) + '_Mid_' + str(num_mid_frames)\n",
    "    data_dir = 'data/CAMUS_data_Mid' + str(num_mid_frames) + '/'  # Directory changes to include the number of mid frames.\n",
    "    print(data_dir) \n",
    "\n",
    "# Set the checkpoint path by appending the file extension '.pth' to the saveFile name.\n",
    "checkpoint_path = saveFile + '.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeee823",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Import the CardiacDataset class from the data_loader module.\n",
    "# This class is used for loading and handling cardiac dataset images and corresponding masks.\n",
    "from data_loader import CardiacDataset\n",
    "\n",
    "# Print the total number of training images by counting files with 'ED.png' in the specified directory.\n",
    "# The glob function returns all file paths matching the given pattern.\n",
    "print('Total train image Samples=' + str(len(glob(data_dir+\"train/image/*ED.png\"))))\n",
    "\n",
    "# Print the total number of training masks (i.e., labels) by counting files with 'ED.png' in the mask directory.\n",
    "print('Total train image Samples=' + str(len(glob(data_dir+\"train/mask/*ED.png\"))))\n",
    "\n",
    "# Print the total number of validation images (test images) in the specified directory.\n",
    "print('Total val image Samples=' + str(len(glob(data_dir+\"test/image/*ED.png\"))))\n",
    "\n",
    "# Print the total number of validation masks (test labels) in the specified directory.\n",
    "print('Total val mask Samples=' + str(len(glob(data_dir+\"test/mask/*ED.png\"))))\n",
    "\n",
    "# Define a series of augmentation transformations using the Albumentations library.\n",
    "# This transformation will be applied to the images and masks during training.\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),         # Randomly flip the image horizontally with 50% probability.\n",
    "    A.VerticalFlip(p=0.5),           # Randomly flip the image vertically with 50% probability.\n",
    "    A.RandomRotate90(p=0.5),         # Randomly rotate the image by 90 degrees with 50% probability.\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, border_mode=0, rotate_limit=45, p=0.5),  # Randomly apply shift, scale, and rotation to the image with 50% probability.\n",
    "    ToTensorV2()                     # Convert the image and mask to PyTorch tensors for model input.\n",
    "],\n",
    "    additional_targets={              # Apply transformations to additional targets (e.g., image2, mask2).\n",
    "        'image2': 'image',\n",
    "        'mask2': 'mask'\n",
    "    })\n",
    "\n",
    "# Create a DataLoader for the training dataset.\n",
    "# This DataLoader will load the training images and masks from the given directory.\n",
    "# The dataset will be shuffled, and multiple workers will be used to load the data in parallel.\n",
    "trainData = DataLoader(\n",
    "    CardiacDataset(\n",
    "        sorted(glob(data_dir+\"test/image/*ED.png\")),    # Sorted list of training images.\n",
    "        sorted(glob(data_dir+\"test/mask/*ED.png\")),     # Sorted list of training masks.\n",
    "        num_mid_frames=num_mid_frames,                  # Number of intermediate frames used for training.\n",
    "        transform=None                                  # No transformations applied (as augmentations will be done within the dataset).\n",
    "    ),\n",
    "    batch_size=config.trainBatch,          # Batch size for training.\n",
    "    shuffle=config.shuffle_,               # Whether to shuffle the dataset during training.\n",
    "    num_workers=config.num_workers         # Number of workers to load data in parallel.\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the validation/test dataset.\n",
    "# This DataLoader will load the validation images and masks from the given directory.\n",
    "valData = DataLoader(\n",
    "    CardiacDataset(\n",
    "        sorted(glob(data_dir+\"test/image/*ED.png\")),    # Sorted list of validation images.\n",
    "        sorted(glob(data_dir+\"test/mask/*ED.png\")),     # Sorted list of validation masks.\n",
    "        num_mid_frames=num_mid_frames,                  # Number of intermediate frames used for validation.\n",
    "        transform=None                                  # No transformations applied for validation.\n",
    "    ),\n",
    "    batch_size=config.valBatch,           # Batch size for validation.\n",
    "    shuffle=config.shuffle_val,           # Whether to shuffle the dataset during validation.\n",
    "    num_workers=config.num_workers        # Number of workers to load validation data in parallel.\n",
    ")\n",
    "\n",
    "# Retrieve a sample from the training DataLoader.\n",
    "# This will give us the first batch of data from the training set.\n",
    "train_sample = first(trainData)\n",
    "\n",
    "# Print the keys of the train_sample dictionary to inspect the loaded data.\n",
    "print(train_sample.keys()) \n",
    "\n",
    "# Print the shape of the 'image' and 'mask' for the first sample in the training set.\n",
    "# This will show the dimensions of the input image and corresponding mask.\n",
    "print('train ED img  ' + str(train_sample[list(train_sample.keys())[0]]['image'].shape))\n",
    "print('train ED mask   ' + str(train_sample[list(train_sample.keys())[0]]['mask'].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e95ba5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Usage for visualizing a sample from the training dataset\n",
    "# The function visualize_dataset_sample will visualize a sample from the 'trainData' DataLoader.\n",
    "\n",
    "visualize_dataset_sample(\n",
    "    trainData,                   # The DataLoader for the training dataset, which provides batches of training data.\n",
    "    list(train_sample.keys()),    # The keys of the train_sample dictionary, which typically contain 'image' and 'mask'.\n",
    "    device='cpu',                # The device (CPU or GPU) to perform the visualization on. 'cpu' is specified here.\n",
    "    num_classes=4,               # The number of classes (e.g., segmentation classes). In this case, it's set to 4.\n",
    "    img_size=config.img_size,    # The image size that the input images should be resized to. Defined in the config.\n",
    "    dataset_type='Train'         # The type of dataset being visualized. This is set to 'Train' to specify it's a training dataset.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e6111",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Usage for visualizing a sample from the validation dataset\n",
    "# The function visualize_dataset_sample will visualize a sample from the 'valData' DataLoader.\n",
    "\n",
    "visualize_dataset_sample(\n",
    "    valData,                      # The DataLoader for the validation dataset, which provides batches of validation data.\n",
    "    list(train_sample.keys()),     # The keys of the train_sample dictionary, which typically contain 'image' and 'mask'.\n",
    "    device='cpu',                 # The device (CPU or GPU) to perform the visualization on. 'cpu' is specified here.\n",
    "    num_classes=4,                # The number of classes (e.g., segmentation classes). In this case, it's set to 4.\n",
    "    img_size=config.img_size,     # The image size that the input images should be resized to. Defined in the config.\n",
    "    dataset_type='Validation'     # The type of dataset being visualized. This is set to 'Validation' to specify it's a validation dataset.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08a131",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolutional block:\n",
    "    This block consists of two 3x3 convolutional layers, each followed by batch normalization \n",
    "    and ReLU activation. It is commonly used in CNN architectures to extract hierarchical features \n",
    "    and stabilize training through normalization.\n",
    "\"\"\"\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        \"\"\"\n",
    "        Initializes the convolutional block.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_channels: int, the number of input channels (e.g., 3 for RGB images).\n",
    "        - output_channels: int, the number of output channels (the number of filters for this layer).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First convolutional layer (3x3 filter, padding to preserve spatial dimensions)\n",
    "        self.conv_layer1 = nn.Conv2d(input_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(output_channels)  # Batch normalization for the output of conv_layer1\n",
    "\n",
    "        # Second convolutional layer (3x3 filter, padding to preserve spatial dimensions)\n",
    "        self.conv_layer2 = nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(output_channels)  # Batch normalization for the output of conv_layer2\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the convolutional block.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Tensor, input tensor to be passed through the convolutional block.\n",
    "        \n",
    "        Returns:\n",
    "        - x: Tensor, the processed output tensor after passing through convolution, batch normalization, and activation.\n",
    "        \"\"\"\n",
    "        # Pass the input through the first convolutional layer, batch normalization, and ReLU activation\n",
    "        x = self.conv_layer1(x)  # Apply the first convolution\n",
    "        x = self.batch_norm1(x)   # Apply batch normalization\n",
    "        x = self.relu(x)          # Apply ReLU activation\n",
    "\n",
    "        # Pass the result through the second convolutional layer, batch normalization, and ReLU activation\n",
    "        x = self.conv_layer2(x)   # Apply the second convolution\n",
    "        x = self.batch_norm2(x)   # Apply batch normalization\n",
    "        x = self.relu(x)          # Apply ReLU activation\n",
    "\n",
    "        return x  # Return the final processed tensor\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Encoder block:\n",
    "    This block consists of a convolutional block followed by a max pooling layer.\n",
    "    The number of filters doubles and the spatial dimensions (height and width) are halved after every block.\n",
    "\"\"\"\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        \"\"\"\n",
    "        Initializes the encoder block.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_channels: int, the number of input channels (e.g., 3 for RGB images).\n",
    "        - output_channels: int, the number of output channels (the number of filters for this layer).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the convolutional block\n",
    "        self.conv_block = ConvBlock(input_channels, output_channels)\n",
    "        \n",
    "        # Initialize max pooling layer (2x2 kernel to halve the spatial dimensions)\n",
    "        self.max_pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder block.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Tensor, input tensor to be passed through the encoder block.\n",
    "        \n",
    "        Returns:\n",
    "        - conv_output: Tensor, the output from the convolutional block.\n",
    "        - pool_output: Tensor, the output from the max pooling operation.\n",
    "        \"\"\"\n",
    "        # Pass input through the convolutional block\n",
    "        conv_output = self.conv_block(x)\n",
    "        \n",
    "        # Pass the output through the max pooling layer\n",
    "        pool_output = self.max_pool(conv_output)\n",
    "\n",
    "        return conv_output, pool_output  # Return both convolutional output and pooled output\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Decoder block:\n",
    "    The decoder block begins with a transpose convolution, followed by a concatenation with the skip\n",
    "    connection from the encoder block. After concatenation, a convolutional block is applied.\n",
    "    In this block, the number of filters decreases by half and the spatial dimensions (height and width) double.\n",
    "\"\"\"\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        \"\"\"\n",
    "        Initializes the decoder block.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_channels: int, the number of input channels (output of the previous decoder or encoder block).\n",
    "        - output_channels: int, the number of output channels (number of filters for the convolutional block).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Transpose convolution for upsampling (doubles the spatial dimensions)\n",
    "        self.up_conv = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Convolutional block after upsampling and concatenation\n",
    "        self.conv_block = ConvBlock(output_channels * 2, output_channels)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder block.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Tensor, input tensor to be passed through the decoder block (usually the output of the previous decoder block).\n",
    "        - skip_connection: Tensor, skip connection from the encoder block to be concatenated after upsampling.\n",
    "        \n",
    "        Returns:\n",
    "        - x: Tensor, the processed output tensor after upsampling, concatenation, and convolution.\n",
    "        \"\"\"\n",
    "        # Upsample the input tensor using transpose convolution (also known as a deconvolution)\n",
    "        x = self.up_conv(x)\n",
    "\n",
    "        # Concatenate the skip connection with the upsampled output along the channel axis (axis=1)\n",
    "        x = torch.cat([x, skip_connection], dim=1)  # Concatenate along the channel dimension\n",
    "\n",
    "        # Apply convolution block after concatenation\n",
    "        x = self.conv_block(x)\n",
    "\n",
    "        return x  # Return the final processed tensor\n",
    "\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block for feature extraction from input frames.\n",
    "    This encoder processes multiple frames independently, and applies attention mechanism at the bottleneck.\n",
    "    \n",
    "    The encoder is structured as follows:\n",
    "    1. Multiple encoder blocks to progressively extract features.\n",
    "    2. A bottleneck block for further feature extraction.\n",
    "    3. An attention mechanism applied to the pooled features at the bottleneck.\n",
    "\n",
    "    Args:\n",
    "        input_depth (int): Number of input channels (default: 1).\n",
    "        features (list): List of feature sizes for each encoder block (default: [64, 128, 256, 512, 1024]).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_depth=1, features=[64, 128, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder Blocks\n",
    "        self.encoder_block1 = EncoderBlock(input_depth, features[0])\n",
    "        self.encoder_block2 = EncoderBlock(features[0], features[1])\n",
    "        self.encoder_block3 = EncoderBlock(features[1], features[2])\n",
    "        self.encoder_block4 = EncoderBlock(features[2], features[3])\n",
    "\n",
    "        # Bottleneck Block\n",
    "        self.bottleneck = ConvBlock(features[3], features[4])\n",
    "\n",
    "        # Attention Mechanism (applied on pooled features)\n",
    "        self.attention_bottleneck = AttentionKQV(all_channels=features[4], embedding_dim=features[4])\n",
    "\n",
    "    def forward(self, *frames):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder. Processes multiple frames and applies attention at the bottleneck.\n",
    "        \n",
    "        Args:\n",
    "            frames (tuple): Multiple frames to be processed independently.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing outputs from encoder stages and attention-enhanced bottleneck.\n",
    "        \"\"\"\n",
    "        # Initialize lists to store outputs for each frame\n",
    "        stage1_outputs, stage2_outputs, stage3_outputs, stage4_outputs, bottleneck_outputs = [], [], [], [], []\n",
    "\n",
    "        # Process each frame independently\n",
    "        for frame in frames:\n",
    "            # Encoder Stage 1\n",
    "            stage1, pool1 = self.encoder_block1(frame)\n",
    "            \n",
    "            # Encoder Stage 2\n",
    "            stage2, pool2 = self.encoder_block2(pool1)\n",
    "            \n",
    "            # Encoder Stage 3\n",
    "            stage3, pool3 = self.encoder_block3(pool2)\n",
    "            \n",
    "            # Encoder Stage 4\n",
    "            stage4, pool4 = self.encoder_block4(pool3)\n",
    "            \n",
    "            # Bottleneck Stage\n",
    "            bottleneck = self.bottleneck(pool4)\n",
    "            \n",
    "            # Collect outputs for each frame\n",
    "            stage1_outputs.append(stage1)\n",
    "            stage2_outputs.append(stage2)\n",
    "            stage3_outputs.append(stage3)\n",
    "            stage4_outputs.append(stage4)\n",
    "            bottleneck_outputs.append(bottleneck)\n",
    "\n",
    "        # Stack bottleneck outputs into a tensor for attention mechanism\n",
    "        bottleneck_outputs = torch.stack(bottleneck_outputs, dim=0)\n",
    "\n",
    "        # Apply attention mechanism on the stacked bottleneck outputs\n",
    "        attention_bottleneck_outputs = self.attention_bottleneck(bottleneck_outputs)\n",
    "\n",
    "        # Flatten the outputs into a tuple (for easy processing downstream)\n",
    "        outputs = tuple(\n",
    "            item for sublist in zip(stage1_outputs, stage2_outputs, stage3_outputs, stage4_outputs, attention_bottleneck_outputs)\n",
    "            for item in sublist\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class AttentionKQV(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism applied on a set of frames using Query, Key, and Value (QKV) attention.\n",
    "    The mechanism computes attention for each frame using other frames as context, applies gating,\n",
    "    and combines the features for further processing.\n",
    "\n",
    "    Args:\n",
    "        all_channels (int): Number of input channels for the frames (default: 1024).\n",
    "        embedding_dim (int): The dimension of the embeddings used for attention (default: 1024).\n",
    "        num_heads (int): Number of attention heads for multi-head attention (default: 8).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, all_channels=1024, embedding_dim=1024, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layers for Query, Key, and Value projections\n",
    "        self.query_linear = nn.Linear(all_channels, embedding_dim, bias=False)\n",
    "        self.key_linear = nn.Linear(all_channels, embedding_dim, bias=False)\n",
    "        self.value_linear = nn.Linear(all_channels, embedding_dim, bias=False)\n",
    "\n",
    "        # Multi-Head Self-Attention\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Gating mechanism (using convolution)\n",
    "        self.gate_conv = nn.Conv2d(all_channels, 1, kernel_size=1, bias=False)\n",
    "        self.gate_activation = nn.Sigmoid()\n",
    "\n",
    "        # Convolutional layer for combining attention and original features\n",
    "        self.combine_conv = nn.Conv2d(all_channels * 2, all_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        # Batch normalization and activation functions\n",
    "        self.batch_norm = nn.BatchNorm2d(all_channels)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Final classifier to process the output\n",
    "        self.classifier = nn.Conv2d(all_channels, all_channels, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, frames):\n",
    "        \"\"\"\n",
    "        Forward pass through the attention mechanism for a set of frames.\n",
    "\n",
    "        Args:\n",
    "            frames (list of torch.Tensor): List of frames (tensors) to be processed.\n",
    "\n",
    "        Returns:\n",
    "            list of torch.Tensor: List of outputs after applying attention and classification.\n",
    "        \"\"\"\n",
    "        num_frames = len(frames)\n",
    "        feature_size = frames[0].size()[2:]  # Get the spatial dimensions (H, W)\n",
    "        all_dim = feature_size[0] * feature_size[1]  # Total spatial size (H*W)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # Process each frame\n",
    "        for i in range(num_frames):\n",
    "            combined_output = 0  # Initialize the accumulated output for the current frame\n",
    "            \n",
    "            # Cross-attention with other frames\n",
    "            for j in range(num_frames):\n",
    "                if i == j:\n",
    "                    continue  # Skip self-attention (no attention to itself)\n",
    "\n",
    "                # Flatten spatial dimensions of both frames for attention computation\n",
    "                frame_i_flat = frames[i].view(-1, frames[i].size(1), all_dim).transpose(1, 2)  # N, H*W, C\n",
    "                frame_j_flat = frames[j].view(-1, frames[j].size(1), all_dim).transpose(1, 2)  # N, H*W, C\n",
    "\n",
    "                # Compute Query, Key, and Value for attention\n",
    "                query = self.query_linear(frame_j_flat)  # N, H*W, Embedding_Dim\n",
    "                key = self.key_linear(frame_i_flat)      # N, H*W, Embedding_Dim\n",
    "                value = self.value_linear(frame_i_flat)  # N, H*W, Embedding_Dim\n",
    "\n",
    "                # Apply Multi-Head Self-Attention\n",
    "                attn_output, _ = self.self_attention(query, key, value)  # N, H*W, Embedding_Dim\n",
    "\n",
    "                # Reshape the attention output to match the spatial dimensions of the frame\n",
    "                attn_output = attn_output.transpose(1, 2).view(-1, frames[i].size(1), feature_size[0], feature_size[1])  # N, C, H, W\n",
    "\n",
    "                # Apply gating mechanism (sigmoid to create a mask)\n",
    "                mask = self.gate_conv(attn_output)\n",
    "                mask = self.gate_activation(mask)\n",
    "                attn_output = attn_output * mask  # Apply the mask to the attention output\n",
    "\n",
    "                # Concatenate the original frame and the gated attention output\n",
    "                combined = torch.cat([attn_output, frames[i]], 1)  # N, 2C, H, W\n",
    "\n",
    "                # Apply convolution, batch normalization, and activation to the combined features\n",
    "                combined = self.combine_conv(combined)\n",
    "                combined = self.batch_norm(combined)\n",
    "                combined = self.activation(combined)\n",
    "\n",
    "                # Accumulate the results\n",
    "                combined_output += combined\n",
    "\n",
    "            # Average the contributions from all other frames\n",
    "            combined_output /= (num_frames - 1)\n",
    "            \n",
    "            # Apply the final classifier\n",
    "            output = self.classifier(combined_output)\n",
    "            outputs.append(output)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class TAM_FCN8s(nn.Module):\n",
    "    def __init__(self, num_classes, feature_depths=[64, 128, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the encoder (Assuming `encoder` is pre-defined)\n",
    "        self.encoder = Encoder(features=feature_depths)\n",
    "        \n",
    "        # Define the classification layer for the bottleneck\n",
    "        self.score_fr = nn.Conv2d(1024, num_classes, kernel_size=1, padding=0)\n",
    "\n",
    "        # Upsampling layers to progressively upsample the features\n",
    "        self.upconv1 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1)\n",
    "        # self.upconv2 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=16, stride=8, padding=4)\n",
    "        \n",
    "        # Skip connection layers (downsampled encoder outputs to be concatenated later)\n",
    "        self.skip_conv1 = nn.Conv2d(feature_depths[1], num_classes, kernel_size=1)\n",
    "        self.skip_conv2 = nn.Conv2d(feature_depths[2], num_classes, kernel_size=1)\n",
    "        self.skip_conv3 = nn.Conv2d(feature_depths[3], num_classes, kernel_size=1)\n",
    "\n",
    "        # Classifier layer to produce the final segmentation mask\n",
    "        self.classifier = nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        num_frames = len(inputs)\n",
    "        masks = []\n",
    "        \n",
    "        # Get the output from the encoder for all frames\n",
    "        encoder_outputs = self.encoder(*inputs)\n",
    "\n",
    "        for i in range(num_frames):\n",
    "            # Unpack encoder outputs for each frame\n",
    "            skip1, skip2, skip3, skip4, bottleneck = encoder_outputs[i * 5:(i + 1) * 5]\n",
    "                 \n",
    "            # Apply classification layer to bottleneck feature map\n",
    "            x = self.score_fr(bottleneck)\n",
    "            \n",
    "            # Upsample the bottleneck features\n",
    "            x = self.upconv1(x)\n",
    "\n",
    "            # Add skip connection from the last encoder stage (skip4)\n",
    "            x = x + self.skip_conv3(skip4)\n",
    "\n",
    "            # Further upsampling to match the original input size\n",
    "            x = self.upconv3(x)\n",
    "\n",
    "            # Apply final classification layer\n",
    "            mask = self.classifier(x)\n",
    "            mask = self.softmax(mask)  # Apply softmax for multi-class segmentation\n",
    "\n",
    "            # Collect the output mask for each frame\n",
    "            masks.append(mask)\n",
    "\n",
    "        return tuple(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebf6c0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = TAM_FCN8s(num_classes=4)\n",
    "\n",
    "# Example input dimensions\n",
    "batch_size = 1\n",
    "channels = 1\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "# Example input data (three frames in this case)\n",
    "inputs = [\n",
    "    torch.randn(batch_size, channels, height, width), \n",
    "    torch.randn(batch_size, channels, height, width),\n",
    "    torch.randn(batch_size, channels, height, width)\n",
    "]\n",
    "\n",
    "# Forward pass to get outputs\n",
    "outputs = model(*inputs)\n",
    "\n",
    "# Print output shape for the first frame\n",
    "print(outputs[0].shape)\n",
    "\n",
    "# Print model summary\n",
    "summary(\n",
    "    model,\n",
    "    input_size=[(batch_size, channels, height, width), \n",
    "                (batch_size, channels, height, width),\n",
    "                (batch_size, channels, height, width)],  # Assuming 3 inputs\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
    "    depth=50,  # Adjust depth to show nested layers\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1406622",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if num_mid_frames is not None:\n",
    "    # Create a list of input tensors based on the number of frames (num_mid_frames + 2)\n",
    "    # Typically, num_mid_frames might represent some intermediate frames between the input and output frames\n",
    "    input_tensors = [torch.randn(1, 1, 256, 256).to(device) for _ in range(num_mid_frames + 2)]\n",
    "\n",
    "    # Perform FLOP count analysis using the input tensors and the model\n",
    "    flops = FlopCountAnalysis(model.to(device), tuple(input_tensors))\n",
    "    \n",
    "    # Print the total FLOPs (both forward and backward passes)\n",
    "    print(f\"FLOPs: {flops.total()}\")\n",
    "    \n",
    "    # Convert the total FLOPs to GFLOPs and print the result\n",
    "    total_flops = flops.total() * 2 / 10**9  # Multiply by 2 for forward + backward and divide by 10^9 for GFLOPs\n",
    "    print(f\"Total FLOPs (forward + backward): {total_flops}\")\n",
    "\n",
    "if num_mid_frames is None:\n",
    "    # Create only 2 input tensors when num_mid_frames is None, typically representing a simpler case (e.g., one frame)\n",
    "    input_tensors = [torch.randn(1, 1, 256, 256).to(device) for _ in range(2)]\n",
    "    \n",
    "    # Perform FLOP count analysis using the input tensors and the model\n",
    "    flops = FlopCountAnalysis(model.to(device), tuple(input_tensors))\n",
    "    \n",
    "    # Print the total FLOPs (both forward and backward passes)\n",
    "    print(f\"FLOPs: {flops.total()}\")\n",
    "    \n",
    "    # Convert the total FLOPs to GFLOPs and print the result\n",
    "    total_flops = flops.total() * 2 / 10**9  # Multiply by 2 for forward + backward and divide by 10^9 for GFLOPs\n",
    "    print(f\"Total FLOPs (forward + backward): {total_flops}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights from a checkpoint\n",
    "# Ensure the model is loaded onto the correct device (CPU or GPU)\n",
    "# model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "# Move the model to the specified device (GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# If double GPU configuration is specified in the config, wrap the model with DataParallel\n",
    "if config.double_GPU:\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1, 2, 3], output_device=[0, 1, 2, 3])\n",
    "\n",
    "# Initialize the Adam optimizer with model parameters and specified learning rate from the config\n",
    "optimizer = torch.optim.Adam(model.parameters(), config.LR)\n",
    "\n",
    "# Lists to track metrics for training and validation phases\n",
    "train_myo_dsc = []\n",
    "train_endo_dsc = []\n",
    "train_epi_dsc = []\n",
    "train_LA_dsc = []\n",
    "train_loss_history = []\n",
    "\n",
    "val_myo_dsc = []\n",
    "val_endo_dsc = []\n",
    "val_epi_dsc = []\n",
    "val_LA_dsc = []\n",
    "val_loss_history = []\n",
    "\n",
    "# Variable to store the best validation DSC for model checkpointing\n",
    "best_val_dsc = 0\n",
    "\n",
    "# Loop over the specified number of epochs for training\n",
    "for epoch in range(config.Epochs):\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Epoch {epoch + 1}/{config.Epochs}\")\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize variables for tracking metrics during the epoch\n",
    "    batch_count = 0\n",
    "    epoch_train_loss = 0\n",
    "    epoch_train_myo_dsc = 0\n",
    "    epoch_train_LA_dsc = 0\n",
    "    epoch_train_endo_dsc = 0\n",
    "    epoch_train_epi_dsc = 0\n",
    "\n",
    "    # Loop over the training data batches\n",
    "    for batch_data in tqdm(trainData):\n",
    "        batch_count += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract the frame keys from batch data\n",
    "        frame_keys = list(batch_data.keys())\n",
    "\n",
    "        # Prepare the input tensors and the corresponding ground truth masks\n",
    "        inputs = [batch_data[key]['image'].to(device) for key in frame_keys]\n",
    "        mask_true = [batch_data[key]['mask'].to(device) for key in frame_keys]\n",
    "\n",
    "        # Ensure the number of inputs matches the expected count for the model\n",
    "        assert len(inputs) == len(frame_keys), f\"Expected {len(frame_keys)} inputs, but got {len(inputs)}\"\n",
    "\n",
    "        # Pass the inputs through the model to get predicted masks\n",
    "        masks = model(*inputs)\n",
    "\n",
    "        # Lists to store evaluation metrics for the current batch\n",
    "        _dscs = []\n",
    "        _mses = []\n",
    "        _myo_endos = []\n",
    "        _epis = []\n",
    "        \n",
    "        # Loop over the masks for each frame\n",
    "        for frame_ in range(len(masks)):\n",
    "            target_dsc, target_mse, target_myo_endo, target_epi = train_helper(masks[frame_], mask_true[frame_], device)\n",
    "            _dscs.append(target_dsc)\n",
    "            _mses.append(target_mse)\n",
    "            _myo_endos.append(target_myo_endo)\n",
    "            _epis.append(target_epi)\n",
    "\n",
    "        # Compute the total loss for the batch and backpropagate\n",
    "        batch_train_loss = sum(_dscs) + sum(_mses)\n",
    "        batch_train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and metrics for the epoch\n",
    "        epoch_train_loss += batch_train_loss.item()\n",
    "        epoch_train_myo_dsc += _myo_endos[0][1].item()\n",
    "        epoch_train_endo_dsc += _myo_endos[0][2].item()\n",
    "        epoch_train_myo_dsc += _myo_endos[1][1].item()\n",
    "        epoch_train_endo_dsc += _myo_endos[1][2].item()\n",
    "        epoch_train_LA_dsc += _myo_endos[0][3].item()\n",
    "        epoch_train_LA_dsc += _myo_endos[1][3].item()\n",
    "        epoch_train_epi_dsc += _epis[0][1].item()\n",
    "        epoch_train_epi_dsc += _epis[1][1].item()\n",
    "\n",
    "    # Average the metrics over the number of batches for this epoch\n",
    "    epoch_train_loss /= batch_count\n",
    "    epoch_train_myo_dsc /= 2 * batch_count\n",
    "    epoch_train_endo_dsc /= 2 * batch_count\n",
    "    epoch_train_epi_dsc /= 2 * batch_count\n",
    "    epoch_train_LA_dsc /= 2 * batch_count\n",
    "\n",
    "    # Store the metrics for this epoch\n",
    "    train_myo_dsc.append(epoch_train_myo_dsc)\n",
    "    train_endo_dsc.append(epoch_train_endo_dsc)\n",
    "    train_epi_dsc.append(epoch_train_epi_dsc)\n",
    "    train_LA_dsc.append(epoch_train_LA_dsc)\n",
    "    train_loss_history.append(epoch_train_loss)\n",
    "\n",
    "    # Validation phase after every 'val_interval' epochs or in the first epoch\n",
    "    if (epoch + 1) % config.val_interval == 0 or epoch == 0:\n",
    "        # Initialize variables for validation phase\n",
    "        val_batch_count = 0\n",
    "        epoch_val_loss = 0\n",
    "        epoch_val_myo_dsc = 0\n",
    "        epoch_val_endo_dsc = 0\n",
    "        epoch_val_LA_dsc = 0\n",
    "        epoch_val_epi_dsc = 0\n",
    "\n",
    "        # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "        model.eval()\n",
    "\n",
    "        # Disable gradient computation for validation to save memory\n",
    "        with torch.no_grad():\n",
    "            for batch_data in tqdm(valData):\n",
    "                val_batch_count += 1\n",
    "                \n",
    "                # Extract frame keys for the validation batch\n",
    "                frame_keys = list(batch_data.keys())\n",
    "\n",
    "                # Prepare the validation input tensors and ground truth masks\n",
    "                inputs = [batch_data[key]['image'].to(device) for key in frame_keys]\n",
    "                mask_true = [batch_data[key]['mask'].to(device) for key in frame_keys]\n",
    "\n",
    "                # Ensure the number of inputs matches the expected count for the model\n",
    "                assert len(inputs) == len(frame_keys), f\"Expected {len(frame_keys)} inputs, but got {len(inputs)}\"\n",
    "\n",
    "                # Pass the validation inputs through the model\n",
    "                masks = model(*inputs)\n",
    "\n",
    "                # Lists to store evaluation metrics for validation\n",
    "                _dscs = []\n",
    "                _mses = []\n",
    "                _myo_endos = []\n",
    "                _epis = []\n",
    "\n",
    "                # Loop over each frame in the validation batch\n",
    "                for frame_ in range(len(masks)):\n",
    "                    target_dsc, target_mse, target_myo_endo, target_epi = train_helper(masks[frame_], mask_true[frame_], device)\n",
    "                    _dscs.append(target_dsc)\n",
    "                    _mses.append(target_mse)\n",
    "                    _myo_endos.append(target_myo_endo)\n",
    "                    _epis.append(target_epi)\n",
    "\n",
    "                # Calculate the total validation loss for the batch\n",
    "                batch_val_loss = sum(_dscs) + sum(_mses)\n",
    "\n",
    "                # Accumulate validation metrics\n",
    "                epoch_val_loss += batch_val_loss.item()\n",
    "                epoch_val_myo_dsc += _myo_endos[0][1].item() + _myo_endos[1][1].item()\n",
    "                epoch_val_endo_dsc += _myo_endos[0][2].item() + _myo_endos[1][2].item()\n",
    "                epoch_val_LA_dsc += _myo_endos[0][3].item() + _myo_endos[1][3].item()\n",
    "                epoch_val_epi_dsc += _epis[0][1].item() + _epis[1][1].item()\n",
    "\n",
    "            # Average validation metrics over the batch count\n",
    "            epoch_val_loss /= val_batch_count\n",
    "            epoch_val_myo_dsc /= 2 * val_batch_count\n",
    "            epoch_val_endo_dsc /= 2 * val_batch_count\n",
    "            epoch_val_LA_dsc /= 2 * val_batch_count\n",
    "            epoch_val_epi_dsc /= 2 * val_batch_count\n",
    "\n",
    "            # Store the validation metrics for this epoch\n",
    "            val_loss_history.append(epoch_val_loss)\n",
    "            val_myo_dsc.append(epoch_val_myo_dsc)\n",
    "            val_endo_dsc.append(epoch_val_endo_dsc)\n",
    "            val_epi_dsc.append(epoch_val_epi_dsc)\n",
    "            val_LA_dsc.append(epoch_val_LA_dsc)\n",
    "\n",
    "            # Calculate the overall average DSC for checkpointing\n",
    "            epoch_val_avg_dsc = (epoch_val_epi_dsc + epoch_val_endo_dsc) / 2\n",
    "\n",
    "            # Save the model if validation DSC improves\n",
    "            if epoch_val_avg_dsc > best_val_dsc:\n",
    "                best_val_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(checkpoint_path))\n",
    "                print(f\"Valid DSC improved from {best_val_dsc:2.5f} to {epoch_val_avg_dsc:2.5f}! Best model is saving as---> {checkpoint_path}\")\n",
    "                best_val_dsc = epoch_val_avg_dsc\n",
    "\n",
    "                # Print training metrics for the current epoch\n",
    "                print(f\"For epoch: {epoch + 1}, Average train total Loss: {epoch_train_loss:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average train DSC MYO: {epoch_train_myo_dsc:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average train DSC ENDO: {epoch_train_endo_dsc:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average train DSC EPI: {epoch_train_epi_dsc:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average train DSC LA: {epoch_train_LA_dsc:.5f}!\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "                # Print validation metrics for the current epoch\n",
    "                print(f\"For epoch: {epoch + 1}, Average validation total Loss: {epoch_val_loss:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average validation DSC MYO: {epoch_val_myo_dsc:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average validation DSC ENDO: {epoch_val_endo_dsc:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average validation DSC EPI: {epoch_val_epi_dsc:.5f}!\")\n",
    "                print(f\"For epoch: {epoch + 1}, Average validation DSC LA: {epoch_val_LA_dsc:.5f}!\")\n",
    "\n",
    "            # Print the best model at the end of the training\n",
    "            print()\n",
    "            print(f'Best model at the epoch of {best_val_epoch:2.0f}, having DSC of {best_val_dsc:2.4f}!!')\n",
    "\n",
    "# Save the training and validation metrics as a DataFrame\n",
    "df = {\n",
    "    'train_myo_dsc': np.array(train_myo_dsc),\n",
    "    'val_myo_dsc': np.array(val_myo_dsc),\n",
    "    'train_endo_dsc': np.array(train_endo_dsc),\n",
    "    'val_endo_dsc': np.array(val_endo_dsc),\n",
    "    'train_epi_dsc': np.array(train_epi_dsc),\n",
    "    'val_epi_dsc': np.array(val_epi_dsc),\n",
    "    'train_LA_dsc': np.array(train_LA_dsc),\n",
    "    'val_LA_dsc': np.array(val_LA_dsc),\n",
    "    'train_loss_history': np.array(train_loss_history),\n",
    "    'val_loss_history': np.array(val_loss_history)\n",
    "}\n",
    "\n",
    "# Save the metrics to a CSV file\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(\"model_training_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68492e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the specified device (GPU or CPU)\n",
    "# Assuming 'device' is defined earlier in the script\n",
    "model = TAM_FCN8s(num_classes=4)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Load the model weights from the checkpoint\n",
    "# The model's state_dict will be loaded onto the appropriate device\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "# Create the directory for saving output if it doesn't already exist\n",
    "if not os.path.exists(saveFile):\n",
    "    os.makedirs(saveFile)\n",
    "\n",
    "# Define the validation/test dataset and DataLoader\n",
    "valData = DataLoader(\n",
    "    CardiacDataset(\n",
    "        # Sorting and loading images and masks for the validation dataset\n",
    "        sorted(glob(data_dir + \"test/image/*ED.png\")),\n",
    "        sorted(glob(data_dir + \"test/mask/*ED.png\")),\n",
    "        num_mid_frames=num_mid_frames,\n",
    "        transform=None  # No transformation applied\n",
    "    ),\n",
    "    batch_size=1,  # Process one image at a time\n",
    "    shuffle=config.shuffle_val,  # Shuffle the dataset if specified\n",
    "    num_workers=config.num_workers  # Number of parallel workers for data loading\n",
    ")\n",
    "\n",
    "# Iterate over batches of data in the validation set\n",
    "for batch_data in tqdm(valData):\n",
    "    \n",
    "    # Extract the frame keys (corresponding to different image/mask files)\n",
    "    frame_keys = list(batch_data.keys())\n",
    "\n",
    "    # Prepare inputs (images) and true masks by moving them to the appropriate device\n",
    "    inputs = [batch_data[key]['image'].to(device) for key in frame_keys]\n",
    "    mask_true = [batch_data[key]['mask'].to(device) for key in frame_keys]\n",
    "\n",
    "    # Ensure that the number of inputs corresponds to the number of expected frames\n",
    "    assert len(inputs) == len(frame_keys), f\"Expected {len(frame_keys)} inputs, but got {len(inputs)}\"\n",
    "\n",
    "    # Pass the prepared inputs to the model to obtain the predicted masks\n",
    "    masks = model(*inputs)\n",
    "\n",
    "    # Iterate through the frames in the batch\n",
    "    for frame_ in range(len(masks)):\n",
    "        # Apply the argmax to the model output to get the predicted mask classes\n",
    "        masks_ = torch.argmax(masks[frame_], dim=1).unsqueeze(1).to(torch.float32)\n",
    "        mask_true_ = mask_true[frame_]\n",
    "        true_img_ = inputs[frame_]\n",
    "\n",
    "        # Modify mask values to specific integers for each class\n",
    "        masks_[masks_ == 1] = 100\n",
    "        masks_[masks_ == 2] = 150\n",
    "        masks_[masks_ == 3] = 200\n",
    "\n",
    "        mask_true_[mask_true_ == 1] = 100\n",
    "        mask_true_[mask_true_ == 2] = 150\n",
    "        mask_true_[mask_true_ == 3] = 200\n",
    "\n",
    "        # Save the output images and masks for each batch and frame\n",
    "        for batch in range(masks_.shape[0]):\n",
    "            # Convert the masks and images to numpy arrays for saving as images\n",
    "            true_mask = mask_true_[batch, :, :].reshape(config.img_size, config.img_size).detach().cpu().numpy()\n",
    "            pred_mask = masks_[batch, :, :].reshape(config.img_size, config.img_size).detach().cpu().numpy()\n",
    "            true_img = true_img_[batch, :, :].reshape(config.img_size, config.img_size).detach().cpu().numpy()\n",
    "\n",
    "            # Save the predicted mask, true mask, and true image as PNG files\n",
    "            cv2.imwrite(saveFile + '/' + os.path.splitext(batch_data['ED']['name'][batch])[0] + frame_keys[frame_] + '_pred_mask.png', pred_mask)\n",
    "            cv2.imwrite(saveFile + '/' + os.path.splitext(batch_data['ED']['name'][batch])[0] + frame_keys[frame_] + '_true_mask.png', true_mask)\n",
    "            cv2.imwrite(saveFile + '/' + os.path.splitext(batch_data['ED']['name'][batch])[0] + frame_keys[frame_] + '_true_image.png', 255 * true_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add63154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "886930dad8d4457679a0ca572c30287744b0c7f7c87d123d0e54902f7e45ac8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
